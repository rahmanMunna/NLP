# `TF-IDF` = Term Frequency â€“ Inverse Document Frequency
## Itâ€™s a numerical statistic used in text mining/NLP to measure how important a word is in a document compared to a collection of documents (the corpus).

1. TF (Term Frequency)
 - Measures how frequently a word appears in a single document.
 - Formula:
   - TF(t,d) = NumberÂ ofÂ timesÂ termÂ **t** appearsÂ inÂ documentÂ dâ€‹ / Total number of terms in document **d**

## ğŸ“Œ Example:

### Document = "The sun is bright. The sun shines."
  - Count of â€œsunâ€ = 2
  - Total words = 6
  - TF("sun") = 2 / 6 = 0.33

2. IDF (Inverse Document Frequency) :
   - Measures how `rare` a word is across all documents.
   - How `importance` a word is across all documents.
   - Words like `"the", "is", "and"` appear everywhere â†’ low importance. (Stop words)
   - Rare words like `"volcano", "hurricane"` â†’ `high importance.`
   - Formula :
     - IDF (t) = log(Number of documents / Number of docuemnts containing t)

## ğŸ“Œ Example:
  - Corpus = `100` documents = `total documents`
  - Word "sun" appears in `5 documents `]
    - IDF("sun") = log(100 / 5) = 2
3. TF-IDF :
  - Final weight = `TF Ã— IDF`
  - **High** `TF-IDF` â†’ word is frequent in a document but rare in the corpus.
  - **Low** TF-IDF â†’ word is either too common (appears everywhere) or absent.

## ğŸ“Œ Example :
  - TF("sun") = 0.33, IDF("sun") = 2
  - TF-IDF("sun") = 0.33 Ã— 2 = 0.66

4. Normalization :
   1. Calculates all `TF-IDF` value for a doc
   2. Squares them , sum them , - takes Square root = `Vector length`
   3. Divides each value by that length

- - - - - -
# ğŸ“Œ Full Example :
 - Here 4 doc :
```
corpus = ['The sky is blue.', # 0
          'The sun is bright today.', # 1
          'The sun in the sky is bright.', # 2
          'We can see the shining sun, the bright sun.'] # 3
```
## Tokenizes your text (splits into words).
```
# Vocabulary
0: 'blue'
1: 'bright'
2: 'can'
3: 'in'
4: 'is'
5: 'see'
6: 'shining'
7: 'sky'
8: 'sun'
9: 'the'
10: 'today'
11: 'we'
```
## Step 1 â€“ Document 0 tokenization
 - Document 0 = "The sky is blue."
 -  Lowercased and tokenized â†’ `["the", "sky", "is", "blue"]`
 -  Total terms = 4
   
## Step 2 â€“ TF (Term Frequency)
 - Term `"sky"` appears `1` time in Doc 0.
 - TF("sky",d0â€‹) = 1 / 4 = .25
   
## Step 3 â€“ IDF (Inverse Document Frequency) : 
 - `"sky"` appears in:
   - Doc 0: âœ…
   - Doc 1: âŒ ("The sun is bright today." no "sky")
   - Doc 2: âœ… ("The sun in the sky is bright.")
   - Doc 3: âŒ
     - So, "sky" appears in 2 documents out of 4.
  - IDF("sky") = log(4/2) = .6932

## Step 4 â€“ TF Ã— IDF (Raw TF-IDF) : 
 - TF-IDF("sky",d0â€‹) = 0.25 Ã— 0.6931 â‰ˆ 0.173275

## Step 5 â€“ Normalization : 
 - By default, TfidfVectorizer normalizes using L2 norm (vector length = 1).
 - We need to do this for all terms in `Document 0` before scaling.
   - Terms in Doc 0 and their raw TF-IDF:
     1. "blue": TF = 0.25, appears in 1 doc â†’ IDF = log(4/1) = log(4) â‰ˆ 1.3863 â†’ raw TF-IDF = 0.25 Ã— 1.3863 â‰ˆ 0.346575
     2. "sky": 0.173275 (calculated above)
     3. "is": TF = 0.25, appears in 3 docs â†’ IDF = log(4/3) â‰ˆ 0.287682 â†’ raw TF-IDF â‰ˆ 0.0719205
     4. "the": TF = 0.25, appears in all docs â†’ IDF = log(4/4) = 0 â†’ raw TF-IDF = 0
## Step 6 â€“ L2 Norm calculation : 
  - Norm = (0.346575)^2 + (0.173275)^2 + (0.0719205)^2 + 0^2
  - Sqrt(Norm)

## Step 7 â€“ Normalized TF-IDF : 
 - Now divide `"sky"â€™s` raw value by the norm:
 - 0.393970 / 173275 â€‹â‰ˆ 0.4398


## Code : 





